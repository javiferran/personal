---
---

@string{aps = {American Physical Society,}}


@inproceedings{https://arxiv.org/pdf/2411.14257,
  abbr= {ArXiv},
  doi = {10.48550/arXiv.2309.02553},
  
  url ={https://openreview.net/forum?id=WCRQFlji2q},
  pdf= {https://openreview.net/forum?id=WCRQFlji2q},
  
  author = {Ferrando*, Javier and Obeso*, Oscar and Rajamanoharan, Senthooran and Nanda, Neel},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
},
  abstract= {Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.},

  publisher = "ArXiv",
  booktitle = "ArXiv",
  
  year = {2024},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://arxiv.org/pdf/2405.00208,
  abbr= {ArXiv},
  doi = {10.48550/arXiv.2309.02553},
  
  url ={https://arxiv.org/abs/2405.00208},
  pdf= {https://arxiv.org/pdf/2405.00208},
  
  author = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and R. Costa-jussà, Marta},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Primer on the Inner Workings of Transformer-based Language Models},
  abstract= {The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.},

  publisher = "ArXiv",
  booktitle = "ArXiv",
  
  year = {2024},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{https://arxiv.org/pdf/2410.06496,
  abbr= {EMNLP Findings},
  doi = {10.48550/arXiv.2309.02553},
  
  url ={https://arxiv.org/abs/2410.06496},
  pdf= {https://arxiv.org/pdf/2410.06496},
  
  author = {Ferrando, Javier and R. Costa-jussà, Marta},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Similarity of Circuits across Languages: a Case Study on the Subject-verb Agreement Task},
  abstract= {Several algorithms implemented by language models have recently been successfully reversed-engineered. However, these findings have been concentrated on specific tasks and models, leaving it unclear how universal circuits are across different settings. In this paper, we study the circuits implemented by Gemma 2B for solving the subject-verb agreement task across two different languages, English and Spanish. We discover that both circuits are highly consistent, being mainly driven by a particular attention head writing a `subject number' signal to the last residual stream, which is read by a small set of neurons in the final MLPs. Notably, this subject number signal is represented as a direction in the residual stream space, and is language-independent. We demonstrate that this direction has a causal effect on the model predictions, effectively flipping the Spanish predicted verb number by intervening with the direction found in English. Finally, we present evidence of similar behavior in other models within the Gemma 1 and Gemma 2 families.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP",
  
  year = {2024},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://arxiv.org/pdf/2403.00824,
  abbr= {EMNLP},
  doi = {10.48550/arXiv.2309.02553},
  
  url ={https://arxiv.org/abs/2403.00824},
  pdf= {https://arxiv.org/pdf/2403.00824},
  
  author = {Ferrando, Javier and Voita, Elena},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Information Flow Routes: Automatically Interpreting Language Models at Scale},
  abstract= {Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
  
  year = {2024},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://arxiv.org/pdf/2404.07004,
  abbr= {ACL Demo},
  doi = {10.48550/arXiv.2309.02553},
  
  url ={https://arxiv.org/abs/2404.07004},
  pdf= {https://arxiv.org/pdf/2404.07004},
  
  author = {Tufnov, Igor and Hambardzumyan, Karen and Ferrando, Javier and Voita, Elena},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models},
  abstract= {We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (1) shows the important part of the whole input-to-output information flow, (2) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (3) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
  
  year = {2024},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://arxiv.org/pdf/2309.04827.pdf,
  abbr= {ACL Findings},
  doi = {10.48550/arXiv.2309.02553},
  
  url ={https://arxiv.org/abs/2309.04827},
  pdf= {https://arxiv.org/pdf/2309.04827.pdf},
  
  author = {Voita, Elena and Ferrando, Javier and Nalmpantis, Christoforos},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neurons in Large Language Models: Dead, N-gram, Positional},
  abstract= {We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70\% in some layers of the 66b model) are "dead", i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
  
  year = {2024},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://arxiv.org/pdf/2309.02553.pdf,
  abbr= {WMT},
  doi = {10.48550/arXiv.2309.02553},
  
  url = {https://arxiv.org/abs/2309.02553},
  pdf= {https://arxiv.org/pdf/2309.02553.pdf},
  
  author = {Ferrando, Javier and Sperber, Matthias and Setiawan, Hendra and Telaar, Dominic and Hasan, Saša},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Automating Behavioral Testing in Machine Translation},
  abstract={Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying only on accuracy.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Eight Conference on Machine Translation (WMT 2023)",
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://doi.org/10.48550/arXiv.2305.12535,
  abbr={ACL},
  doi = {10.48550/arXiv.2305.12535},
  
  url = {https://arxiv.org/abs/2305.12535},
  pdf={https://arxiv.org/pdf/2305.12535.pdf},
  
  author = {Ferrando, Javier and Gállego, Gerard I. and Tsiamas, Ioannis and Costa-jussà, Marta R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Explaining How Transformers Use Context to Build Predictions},
  abstract={Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model's prediction, it is still unclear how prior words affect the model's decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{https://doi.org/10.48550/arxiv.2205.11631,
  abbr={EMNLP},
  doi = {10.48550/ARXIV.2205.11631},
  
  url = {https://arxiv.org/abs/2205.11631},
  pdf={https://arxiv.org/pdf/2205.11631.pdf},
  
  author = {Ferrando, Javier and Gállego, Gerard I. and Alastruey, Belen and Escolano, Carlos and Costa-jussà, Marta R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer},
  abstract={In Neural Machine Translation (NMT), each token prediction is conditioned on the source sentence and the target prefix (what has been previously translated at a decoding step). However, previous work on interpretability in NMT has focused solely on source sentence tokens attributions. Therefore, we lack a full understanding of the influences of every input token (source sentence and target prefix) in the model predictions. In this work, we propose an interpretability method that tracks complete input token attributions. Our method, which can be extended to any encoder-decoder Transformer-based model, allows us to better comprehend the inner workings of current NMT models. We apply the proposed method to both bilingual and multilingual Transformers and present insights into their behaviour.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{https://doi.org/10.48550/arxiv.2203.04212,
  abbr={EMNLP},
  doi = {10.48550/ARXIV.2203.04212},
  
  url = {https://arxiv.org/abs/2203.04212},
  pdf={https://arxiv.org/pdf/2203.04212.pdf},
  author = {Ferrando, Javier and Gállego, Gerard I. and Costa-jussà, Marta R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Measuring the Mixing of Contextual Information in the Transformer},
  abstract={The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block -- multi-head attention, residual connection, and layer normalization -- and define a metric to measure token-to-token interactions within each layer, considering the characteristics of the representation space. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides faithful explanations and outperforms similar aggregation methods.},

  publisher = "Association for Computational Linguistics",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{alastruey-etal-2022-locality,
    abbr={ACL SRW},
    title = "On the Locality of Attention in Direct Speech Translation",
    pdf={https://aclanthology.org/2022.acl-srw.32.pdf},
    author = "Belen Alastruey*  and
      Javier Ferrando*  and
      Gerard I. Gállego  and
      Marta R. Costa-jussá",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-srw.32",
    doi = "10.18653/v1/2022.acl-srw.32",
    pages = "402--412",
    abstract = "Transformers have achieved state-of-the-art results across multiple NLP tasks. However, the self-attention mechanism complexity scales quadratically with the sequence length, creating an obstacle for tasks involving long sequences, like in the speech domain. In this paper, we discuss the usefulness of self-attention for Direct Speech Translation. First, we analyze the layer-wise token contributions in the self-attention of the encoder, unveiling local diagonal patterns. To prove that some attention weights are avoidable, we propose to substitute the standard self-attention with a local efficient one, setting the amount of context used based on the results of the analysis. With this approach, our model matches the baseline performance, and improves the efficiency by skipping the computation of those weights that standard attention discards.",
}

@inproceedings{costajussa:2022,
abbr={AAAI},
  author = {Costa-jussà, Marta R. and Escolano, Carlos and Basta, Christine and Ferrando, Javier and Batlle, Roser and Kharitonova, Ksenia},
  title = {Interpreting Gender Bias in Neural Machine Translation: Architecture Matters},
  abstract = {Multilingual neural machine translation architectures mainly differ in the number of sharing modules and parameters applied among languages. In this paper, and from an algorithmic perspective, we explore whether the chosen architecture, when trained with the same data, influences the level of gender bias. Experiments conducted in three language pairs show that language-specific encoder-decoders exhibit less bias than the shared architecture. We propose two methods for interpreting and studying gender bias in machine translation based on source embeddings and attention. Our analysis shows that, in the language-specific case, the embeddings encode more gender information, and their attention is more diverted. Both behaviors help in mitigating gender bias.},
  booktitle = {Proceedings of AAAI},
  url = "https://www.aaai.org/AAAI22Papers/AISI-2223.CostajussaM.pdf",
  pdf={https://www.aaai.org/AAAI22Papers/AISI-2223.CostajussaM.pdf},
  year = {2022},
}

@inproceedings{ferrando-costa-jussa-2021-attention-weights,
    abbr={EMNLP Findings},
    title = "Attention Weights in Transformer {NMT} Fail Aligning Words Between Sequences but Largely Explain Model Predictions",
    pdf={https://aclanthology.org/2021.findings-emnlp.39.pdf},
    author = "Javier Ferrando and
      Marta R. Costa-jussà",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.39",
    doi = "10.18653/v1/2021.findings-emnlp.39",
    pages = "434--443",
    abstract = "This work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the source sequence. However, we observe that NMT models assign attention to these tokens to regulate the contribution in the prediction of the two contexts, the source and the prefix of the target sequence. We provide evidence about the influence of wrong alignments on the model behavior, demonstrating that the encoder-decoder attention mechanism is well suited as an interpretability method for NMT. Finally, based on our analysis, we propose methods that largely reduce the word alignment error rate compared to standard induced alignments from attention weights.",
}

@InProceedings{10.1007/978-3-030-50417-5_29,
abbr={ICCS},
author="Ferrando, Javier
and Dom{\'i}nguez, Juan Luis
and Torres, Jordi
and Garc{\'i}a, Ra{\'u}l
and Garc{\'i}a, David
and Garrido, Daniel
and Cortada, Jordi
and Valero, Mateo",
editor="Krzhizhanovskaya, Valeria V.
and Z{\'a}vodszky, G{\'a}bor
and Lees, Michael H.
and Dongarra, Jack J.
and Sloot, Peter M. A.
and Brissos, S{\'e}rgio
and Teixeira, Jo{\~a}o",
title="Improving Accuracy and Speeding Up Document Image Classification Through Parallel Systems",
pdf={https://link.springer.com/content/pdf/10.1007/978-3-030-50417-5_29.pdf},
booktitle="Computational Science -- ICCS 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="387--400",
abstract="This paper presents a study showing the benefits of the EfficientNet models compared with heavier Convolutional Neural Networks (CNNs) in the Document Classification task, essential problem in the digitalization process of institutions. We show in the RVL-CDIP dataset that we can improve previous results with a much lighter model and present its transfer learning capabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we present an ensemble pipeline which is able to boost solely image input by combining image model predictions with the ones generated by BERT model on extracted text by OCR. We also show that the batch size can be effectively increased without hindering its accuracy so that the training process can be sped up by parallelizing throughout multiple GPUs, decreasing the computational time needed. Lastly, we expose the training performance differences between PyTorch and Tensorflow Deep Learning frameworks.",
isbn="978-3-030-50417-5"
}
